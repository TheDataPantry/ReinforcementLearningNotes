{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf94901",
   "metadata": {},
   "source": [
    "### The following shows built in environments.\n",
    "\n",
    "They are categorized into several categories like classic control (Cartpole, pendulum) which are canonical environments used in textbooks, Box2D which is a 2D physics engine for games, ToyText which is a small and simple environment used to debug RL algorithms (blackjack), etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59de21ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPole-v0\n",
      "CartPole-v1\n",
      "MountainCar-v0\n",
      "MountainCarContinuous-v0\n",
      "Pendulum-v1\n",
      "Acrobot-v1\n",
      "phys2d/CartPole-v0\n",
      "phys2d/CartPole-v1\n",
      "phys2d/Pendulum-v0\n",
      "LunarLander-v3\n",
      "LunarLanderContinuous-v3\n",
      "BipedalWalker-v3\n",
      "BipedalWalkerHardcore-v3\n",
      "CarRacing-v3\n",
      "Blackjack-v1\n",
      "FrozenLake-v1\n",
      "FrozenLake8x8-v1\n",
      "CliffWalking-v0\n",
      "Taxi-v3\n",
      "tabular/Blackjack-v0\n",
      "tabular/CliffWalking-v0\n",
      "Reacher-v2\n",
      "Reacher-v4\n",
      "Reacher-v5\n",
      "Pusher-v2\n",
      "Pusher-v4\n",
      "Pusher-v5\n",
      "InvertedPendulum-v2\n",
      "InvertedPendulum-v4\n",
      "InvertedPendulum-v5\n",
      "InvertedDoublePendulum-v2\n",
      "InvertedDoublePendulum-v4\n",
      "InvertedDoublePendulum-v5\n",
      "HalfCheetah-v2\n",
      "HalfCheetah-v3\n",
      "HalfCheetah-v4\n",
      "HalfCheetah-v5\n",
      "Hopper-v2\n",
      "Hopper-v3\n",
      "Hopper-v4\n",
      "Hopper-v5\n",
      "Swimmer-v2\n",
      "Swimmer-v3\n",
      "Swimmer-v4\n",
      "Swimmer-v5\n",
      "Walker2d-v2\n",
      "Walker2d-v3\n",
      "Walker2d-v4\n",
      "Walker2d-v5\n",
      "Ant-v2\n",
      "Ant-v3\n",
      "Ant-v4\n",
      "Ant-v5\n",
      "Humanoid-v2\n",
      "Humanoid-v3\n",
      "Humanoid-v4\n",
      "Humanoid-v5\n",
      "HumanoidStandup-v2\n",
      "HumanoidStandup-v4\n",
      "HumanoidStandup-v5\n",
      "GymV21Environment-v0\n",
      "GymV26Environment-v0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "for i in gym.envs.registry.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69f7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87c9d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space:  Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation space: \", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ff252",
   "metadata": {},
   "source": [
    "This observation space has 4 dimensions. Which are:\n",
    "\n",
    "* Cart Position (-4.8, 4.8)\n",
    "* Cart Velocity (- to +)\n",
    "* Pole angle (-0.4189, +0.4189)\n",
    "* Pole angular velocity (- to +)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3283cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hiowa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:  [ 0.0454559   0.00248629  0.04908514 -0.01469923]\n"
     ]
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "print(\"observation: \", observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e062cfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space:  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(\"action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe79eff",
   "metadata": {},
   "source": [
    "This means there are a total of two actions an agent can take.\n",
    "\n",
    "* 0: Push the cart to the left\n",
    "* 1: Push the cart to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f88f454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d6dd646090>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = gym.make(\"CartPole-v1\", render_mode = 'human')\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "SEED = 1111\n",
    "env.reset(seed=SEED)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c406f631",
   "metadata": {},
   "source": [
    "### Using a simple policy gradient agent\n",
    "\n",
    "The code below maps observed states to actions. So given an input observation, it predicts the right action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bd36755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbbbe1a",
   "metadata": {},
   "source": [
    "### Reward collection\n",
    "\n",
    "It is common to adjust future rewards using a discount factor and to normalize the array of stepwaise returns to ensure smooth and stable training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a29812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stepwise_returns(rewards, discount_factor):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + R * discount_factor\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    normalized_returns = (returns - returns.mean()) / returns.std()\n",
    "    return normalized_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf2ba7",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "Runs the agent based on the current policy until it reaches a terminal state and collecting the stepwise rewards and action probabilities. This is done through:\n",
    "\n",
    "* Resetting environment to initial state\n",
    "* initialize buffers to store the action probabilities, rewards, and cumulative return\n",
    "* use the .step() function to iteratively run the agent in the environment until it terminates:\n",
    "    * get the observation of the environment's state\n",
    "    * Get the action predicted by the policy based on the observation\n",
    "    * Use the Softmax function to estimate the probability of taking the predicted action\n",
    "    * simulate a categorical probability distribution based on these estimated probabilities\n",
    "    * Sample the distribution to get the agent's action\n",
    "    * Estimate the log probability of the sampled action from the simulated distribution\n",
    "* Append the log probability of the actions and the rewards from each step to their respective buffers\n",
    "* Estimate the normalized and discounted values of the returns at each step baed on the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b0e9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import distributions\n",
    "def forward_pass(env, policy, discount_factor):\n",
    "    log_prob_actions = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_return = 0\n",
    "    policy.train()\n",
    "    observation, info = env.reset()\n",
    "    while not done:\n",
    "        observation = torch.FloatTensor(observation).unsqueeze(0)\n",
    "        action_pred = policy(observation)\n",
    "        action_prob = F.softmax(action_pred, dim = -1)\n",
    "        dist = distributions.Categorical(action_prob)\n",
    "        action = dist.sample()\n",
    "        log_prob_action = dist.log_prob(action)\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        env.render()\n",
    "        done = terminated or truncated\n",
    "        log_prob_actions.append(log_prob_action)\n",
    "        rewards.append(reward)\n",
    "        episode_return += reward\n",
    "    log_prob_actions = torch.cat(log_prob_actions)\n",
    "    stepwise_returns = calculate_stepwise_returns(rewards, discount_factor)\n",
    "    return episode_return, stepwise_returns, log_prob_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea4dd562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(stepwise_returns, log_prob_actions):\n",
    "    loss = -(stepwise_returns * log_prob_actions).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51b682",
   "metadata": {},
   "source": [
    "### The code below does backpropagation with respect to the loss function above. This updates the policy's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb2d6489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(stepwise_returns, log_prob_actions, optimizer):\n",
    "    stepwise_returns = stepwise_returns.detach()\n",
    "    loss = calculate_loss(stepwise_returns, log_prob_actions)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "937783fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def main(): \n",
    "    MAX_EPOCHS = 500\n",
    "    DISCOUNT_FACTOR = 0.99\n",
    "    N_TRIALS = 25\n",
    "    REWARD_THRESHOLD = 475\n",
    "    PRINT_INTERVAL = 10\n",
    "    INPUT_DIM = env.observation_space.shape[0]\n",
    "    HIDDEN_DIM = 128\n",
    "    OUTPUT_DIM = env.action_space.n\n",
    "    DROPOUT = 0.5\n",
    "    episode_returns = []\n",
    "    policy = PolicyNetwork(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT)\n",
    "    LEARNING_RATE = 0.01\n",
    "    optimizer = optim.Adam(policy.parameters(), lr = LEARNING_RATE)\n",
    "    for episode in range(1, MAX_EPOCHS+1):\n",
    "        episode_return, stepwise_returns, log_prob_actions = forward_pass(env, policy, DISCOUNT_FACTOR)\n",
    "        _ = update_policy(stepwise_returns, log_prob_actions, optimizer)\n",
    "        episode_returns.append(episode_return)\n",
    "        mean_episode_return = np.mean(episode_returns[-N_TRIALS:])\n",
    "        if episode % PRINT_INTERVAL == 0:\n",
    "            print(f'| Episode: {episode:3} | Mean Rewards: {mean_episode_return:5.1f} |')\n",
    "        if mean_episode_return >= REWARD_THRESHOLD:\n",
    "            print(f'Reached reward threshold in {episode} episodes')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ffedaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Episode:  10 | Mean Rewards:  28.8 |\n",
      "| Episode:  20 | Mean Rewards:  26.8 |\n",
      "| Episode:  30 | Mean Rewards:  31.0 |\n",
      "| Episode:  40 | Mean Rewards:  45.6 |\n",
      "| Episode:  50 | Mean Rewards:  62.7 |\n",
      "| Episode:  60 | Mean Rewards:  78.8 |\n",
      "| Episode:  70 | Mean Rewards: 108.1 |\n",
      "| Episode:  80 | Mean Rewards: 129.2 |\n",
      "| Episode:  90 | Mean Rewards: 148.8 |\n",
      "| Episode: 100 | Mean Rewards: 159.8 |\n",
      "| Episode: 110 | Mean Rewards: 130.6 |\n",
      "| Episode: 120 | Mean Rewards: 104.9 |\n",
      "| Episode: 130 | Mean Rewards: 182.3 |\n",
      "| Episode: 140 | Mean Rewards: 206.2 |\n",
      "| Episode: 150 | Mean Rewards: 149.4 |\n",
      "| Episode: 160 | Mean Rewards:  69.2 |\n",
      "| Episode: 170 | Mean Rewards:  81.0 |\n",
      "| Episode: 180 | Mean Rewards: 180.2 |\n",
      "| Episode: 190 | Mean Rewards: 293.8 |\n",
      "| Episode: 200 | Mean Rewards: 380.9 |\n",
      "| Episode: 210 | Mean Rewards: 324.4 |\n",
      "| Episode: 220 | Mean Rewards: 250.0 |\n",
      "| Episode: 230 | Mean Rewards: 222.4 |\n",
      "| Episode: 240 | Mean Rewards: 195.4 |\n",
      "| Episode: 250 | Mean Rewards: 142.5 |\n",
      "| Episode: 260 | Mean Rewards: 138.0 |\n",
      "| Episode: 270 | Mean Rewards: 159.3 |\n",
      "| Episode: 280 | Mean Rewards: 209.1 |\n",
      "| Episode: 290 | Mean Rewards: 245.1 |\n",
      "| Episode: 300 | Mean Rewards: 288.4 |\n",
      "| Episode: 310 | Mean Rewards: 364.9 |\n",
      "| Episode: 320 | Mean Rewards: 444.5 |\n",
      "Reached reward threshold in 324 episodes\n"
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c7772e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
